# SimConfidenceEval - Sim-Real Confidence Evaluation Platform

A professional evaluation framework for measuring the similarity and confidence between simulated and real image data.

## Project Overview

This platform integrates **multiple image similarity evaluation metrics** (classical metrics and deep learning-based metrics) to comprehensively assess paired simulated and real images. It supports flexible configuration systems, batch processing, GPU acceleration, and result visualization.

## Key Features

- ğŸ¯ **Multiple Evaluation Metrics**: Including PSNR, SSIM, FSIM, LPIPS, FID, etc.
- âš¡ **GPU Acceleration**: Full GPU support for batch processing acceleration
- ğŸ“Š **Flexible Configuration**: YAML-based configuration system with command-line override support
- ğŸ“ˆ **Result Visualization**: Generates score distribution histograms, worst-case comparisons, etc.
- ğŸ”§ **Modular Design**: Easy to extend with new metrics and data formats
- ğŸ“ **Comprehensive Logging**: Detailed run logs and configuration backups

## Project Structure

```
SimConfidenceEval/
â”œâ”€â”€ main.py                          # Main entry point script
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ configs/                         # Configuration files directory
â”‚   â”œâ”€â”€ basic_eval.yaml             # Basic evaluation configuration
â”‚   â””â”€â”€ dataset_eval.yaml           # Dataset evaluation configuration
â”œâ”€â”€ data/                            # Data files directory
â”‚   â”œâ”€â”€ sim/                        # Simulated data
â”‚   â”‚   â””â”€â”€ warship/               # Example category: warship
â”‚   â””â”€â”€ real/                       # Real data
â”‚       â””â”€â”€ warship/
â”œâ”€â”€ src/                             # Source code
â”‚   â”œâ”€â”€ core/                       # Core module
â”‚   â”‚   â””â”€â”€ evaluator.py           # Main evaluator class
â”‚   â”œâ”€â”€ data/                       # Data processing module
â”‚   â”‚   â”œâ”€â”€ datasets.py            # Dataset definition and loading
â”‚   â”‚   â”œâ”€â”€ transforms.py          # Data augmentation and transforms
â”‚   â”‚   â””â”€â”€ utils.py               # Data processing utilities
â”‚   â”œâ”€â”€ metrics/                    # Evaluation metrics module
â”‚   â”‚   â”œâ”€â”€ base.py                # Base class definition
â”‚   â”‚   â”œâ”€â”€ classical.py           # Classical metrics (PSNR/SSIM/FSIM)
â”‚   â”‚   â””â”€â”€ deep.py                # Deep learning metrics (LPIPS/FID)
â”‚   â””â”€â”€ utils/                      # Utility module
â”‚       â”œâ”€â”€ logger.py              # Logging utilities
â”‚       â”œâ”€â”€ io.py                  # File I/O utilities
â”‚       â””â”€â”€ visualizer.py          # Visualization utilities
â”œâ”€â”€ test/                            # Test code
â”‚   â””â”€â”€ test_metrics.py            # Metrics unit tests
â””â”€â”€ outputs/                         # Output results directory
    â””â”€â”€ run_001/                    # Example run results
```

## Supported Evaluation Metrics

| Metric | Type | Description |
|--------|------|-------------|
| **PSNR** | Classical | Peak Signal-to-Noise Ratio, higher is better |
| **SSIM** | Classical | Structural Similarity Index, considers luminance, contrast, structure |
| **FSIM** | Classical | Feature Similarity Index, based on phase consistency |
| **LPIPS** | Deep Learning | Learned Perceptual Image Patch Similarity, using pretrained neural networks |
| **FID** | Deep Learning | FrÃ©chet Inception Distance, evaluates overall distribution similarity |

## Quick Start

### 1. Environment Setup

```bash
# Install dependencies
pip install -r requirements.txt

# Or using conda
conda env create -f environment.yml
```

### 2. Prepare Data

Organize simulated and real data in the following structure:

```
data/
â”œâ”€â”€ sim/
â”‚   â””â”€â”€ <category>/
â”‚       â”œâ”€â”€ image1.jpg
â”‚       â”œâ”€â”€ image2.jpg
â”‚       â””â”€â”€ ...
â””â”€â”€ real/
    â””â”€â”€ <category>/
        â”œâ”€â”€ image1.jpg
        â”œâ”€â”€ image2.jpg
        â””â”€â”€ ...
```

### 3. Configure Evaluation Parameters

Edit `configs/basic_eval.yaml`:

```yaml
experiment_name: "my_experiment"
output_dir: "./outputs/run_001"

data:
  mode: "paired"                    # Paired mode
  sim_path: "./data/sim/"
  real_path: "./data/real/"
  iou_threshold: 0.8               # IoU filtering threshold
  batch_size: 32
  image_size: [640, 480]           # [H, W]

metrics:
  - name: "PSNR"
    params:
      psnr_max: 50.0
  - name: "SSIM"
  - name: "LPIPS"
    params:
      net: "vgg"
      lpips_max: 1.0
  - name: "FID"
    params:
      feature: 2048

visualization:
  plot_hist: True                  # Generate histograms
  save_bad_cases: True             # Save worst-case comparisons
```

### 4. Run Evaluation

```bash
# Use default configuration
python main.py

# Specify configuration file
python main.py --config configs/dataset_eval.yaml

# Override output directory
python main.py --config configs/basic_eval.yaml --output ./outputs/custom_run
```

## Output Results

After evaluation completes, the output directory contains:

```
outputs/run_001/
â”œâ”€â”€ config_backup.yaml                    # Configuration file backup
â”œâ”€â”€ final_report_summary.csv              # Summary report (mean, median, etc. for each metric)
â”œâ”€â”€ final_report_detailed.csv             # Detailed report (scores for each image pair)
â”œâ”€â”€ plots/
â”‚   â”œâ”€â”€ score_distribution_PSNR.png      # Score distribution histograms
â”‚   â”œâ”€â”€ score_distribution_SSIM.png
â”‚   â”œâ”€â”€ ...
â”‚   â”œâ”€â”€ worst_10_case_comparison.png     # Worst-case comparisons
â”‚   â””â”€â”€ ...
â””â”€â”€ logs/
    â””â”€â”€ eval_*.log                       # Run logs
```

## Configuration Details

### Data Configuration

- `mode`: Data loading mode
  - `paired`: Paired mode, requires identical directory structures for sim and real data
- `iou_threshold`: Box filtering threshold for detection quality control
- `batch_size`: Batch size for processing, adjust based on GPU memory
- `image_size`: Unified image size `[height, width]`

### Metrics Configuration

Each metric can include `name` and optional `params`:

- **PSNR** Parameters:
  - `psnr_max`: Upper limit for normalization (default: 50)

- **LPIPS** Parameters:
  - `net`: Feature network `alex` / `vgg` / `squeeze`
  - `version`: Model version
  - `lpips_max`: Normalization upper limit (default: 1.0)

- **FID** Parameters:
  - `feature`: Feature dimension `64` / `192` / `768` / `2048`

### Visualization Configuration

- `plot_hist`: Whether to generate score distribution histograms
- `save_bad_cases`: Whether to save worst-case image pairs

## Extension Guide

### Adding New Evaluation Metrics

1. **Create a metric class in `src/metrics/`**

```python
from src.metrics.base import BaseMetric
import torch

class MyMetric(BaseMetric):
    def __init__(self, cfg=None):
        super().__init__(cfg)
        # Initialize your metric
        
    def forward(self, preds, target):
        # Implement computation logic
        score = ...  # Compute similarity
        return self._normalize(score)
    
    def _normalize(self, x):
        # Normalize to [0, 1]
        return x / max_value
```

2. **Register in `src/metrics/__init__.py`**

```python
from .classical import PSNR, SSIM, FSIM
from .deep import LPIPS, FID
from .custom import MyMetric  # Import new metric
```

3. **Use in configuration file**

```yaml
metrics:
  - name: "MyMetric"
    params:
      param1: value1
```

### Supporting New Data Formats

Modify the `build_dataloader` function in `src/data/datasets.py` to support other data formats (e.g., detection box annotations, point clouds, etc.).

## Performance Recommendations

- Use GPU: Ensure CUDA is available for 10-100x speedup
- Batch size adjustment: Adjust `batch_size` based on GPU memory (recommended: 32-128)
- Image size: Smaller sizes for faster processing, larger sizes for better accuracy (recommended: 640x480)
- Metric selection: LPIPS/FID are slower; prioritize classical metrics for quick evaluation

## Dependency Notes

Key dependencies include:

- PyTorch: Deep learning framework
- PyYAML: Configuration file parsing
- scikit-image: Image processing algorithms
- Pillow: Image I/O
- matplotlib: Visualization
- pandas: Data processing

See `requirements.txt` for details.

## FAQ

**Q: How to handle images with inconsistent sizes?**  
A: The framework automatically resizes all images to the `image_size` specified in the configuration.

**Q: What image formats are supported?**  
A: All formats supported by PIL are supported (JPEG, PNG, BMP, etc.).

**Q: How to run on CPU?**  
A: The framework automatically detects CUDA availability and falls back to CPU if GPU is not available.

**Q: How to skip certain metrics?**  
A: Simply delete or comment out unwanted metrics in the configuration file.

## License

MIT

## Contact

For issues or suggestions, please submit an Issue or Pull Request.
